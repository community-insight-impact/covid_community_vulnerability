{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook creates the full merged CII dataset including all varibles from the core metrics and additional variables from CHRRP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import libraries and build helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a function to add leading 0s back to FIPS code (we'll use this at the end)\n",
    "def fips_zeros(fips):\n",
    "    \"\"\"Adds leading 0s back to FIPS codes\"\"\"\n",
    "    fips = str(fips)\n",
    "    current_length = len(fips)\n",
    "    if current_length < 5:\n",
    "        number_of_leading_zeros = 5 - current_length\n",
    "        leading_zeros = ''.join('0' * number_of_leading_zeros)\n",
    "        fips = leading_zeros + fips\n",
    "    return fips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama Autauga County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>Alabama Baldwin County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>Alabama Barbour County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>Alabama Bibb County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>Alabama Blount County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS    State          County                   Merge\n",
       "0  1001  Alabama  Autauga County  Alabama Autauga County\n",
       "1  1003  Alabama  Baldwin County  Alabama Baldwin County\n",
       "2  1005  Alabama  Barbour County  Alabama Barbour County\n",
       "3  1007  Alabama     Bibb County     Alabama Bibb County\n",
       "4  1009  Alabama   Blount County   Alabama Blount County"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read in the base FIPS code dataset and drop Puerto Rico \n",
    "FIPS_codes=pd.read_csv('../data/FIPS.csv', usecols=['FIPS','County','State','Merge'])\n",
    "FIPS_codes=FIPS_codes[FIPS_codes['State']!='Puerto Rico']\n",
    "#FIPS_codes['FIPS']=FIPS_codes['FIPS'].apply(fips_zeros)\n",
    "FIPS_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add in the state abbreviations too for some of the data we'll use later on. Let's define a dictionary to do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPS_codes['State Abbrv']=FIPS_codes['State'].map(us_state_abbrev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Merge</th>\n",
       "      <th>State Abbrv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>Alabama Autauga County</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>Alabama Baldwin County</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>Alabama Barbour County</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>Alabama Bibb County</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>Alabama Blount County</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS    State          County                   Merge State Abbrv\n",
       "0  1001  Alabama  Autauga County  Alabama Autauga County          AL\n",
       "1  1003  Alabama  Baldwin County  Alabama Baldwin County          AL\n",
       "2  1005  Alabama  Barbour County  Alabama Barbour County          AL\n",
       "3  1007  Alabama     Bibb County     Alabama Bibb County          AL\n",
       "4  1009  Alabama   Blount County   Alabama Blount County          AL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIPS_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3,142 counties or county equivalents in the US (including DC but excluding territories. Let's double check that those are all in our base FIPS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3142\n"
     ]
    }
   ],
   "source": [
    "print(FIPS_codes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, they're all there! \n",
    "\n",
    "Write a function to check if all FIPS codes exist in a column. You must first save the unique column values as a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a function to print all missing fips codes, you must first turn the fips column in a series of it's unique values\n",
    "def missing_fips(fips):\n",
    "    \"\"\"\"Checks if all fips codes exist in a dataframe and prints the name of any that don't\"\"\"\n",
    "    #fips=str(fips)\n",
    "    print(\"missing FIPS codes:\")\n",
    "    for i, name in FIPS_codes['FIPS'].iteritems():\n",
    "        #print(name)\n",
    "        if not name in fips:\n",
    "            print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIPS_codes_test=FIPS_codes[FIPS_codes['State']!='Alabama']\n",
    "FIPS_codes_test.head()\n",
    "fips_test_unique=FIPS_codes_test['FIPS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing FIPS codes:\n",
      "1001\n",
      "1003\n",
      "1005\n",
      "1007\n",
      "1009\n",
      "1011\n",
      "1013\n",
      "1015\n",
      "1017\n",
      "1019\n",
      "1021\n",
      "1023\n",
      "1025\n",
      "1027\n",
      "1029\n",
      "1031\n",
      "1033\n",
      "1035\n",
      "1037\n",
      "1039\n",
      "1041\n",
      "1043\n",
      "1045\n",
      "1047\n",
      "1049\n",
      "1051\n",
      "1053\n",
      "1055\n",
      "1057\n",
      "1059\n",
      "1061\n",
      "1063\n",
      "1065\n",
      "1067\n",
      "1069\n",
      "1071\n",
      "1073\n",
      "1075\n",
      "1077\n",
      "1079\n",
      "1081\n",
      "1083\n",
      "1085\n",
      "1087\n",
      "1089\n",
      "1091\n",
      "1093\n",
      "1095\n",
      "1097\n",
      "1099\n",
      "1101\n",
      "1103\n",
      "1105\n",
      "1107\n",
      "1109\n",
      "1111\n",
      "1113\n",
      "1115\n",
      "1117\n",
      "1119\n",
      "1121\n",
      "1123\n",
      "1125\n",
      "1127\n",
      "1129\n",
      "1131\n",
      "1133\n"
     ]
    }
   ],
   "source": [
    "missing_fips(fips_test_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, it works!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the datasets\n",
    "\n",
    "### CHRRP\n",
    "We'll start with the County Health Rankings and Roadmaps data\n",
    "\n",
    "Let's start by selecting the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the first part of the CHRRP dataset\n",
    "cols_1=[0,1,2,3,4,23,27,31,36,55,59,63,65,69,71,77,82,84,109,113,127,134,142,146,152,154,165,169,174,177,180,\n",
    "       199,201,203,216,236]\n",
    "c1_data=pd.read_excel('../data/2020_CHRD.xlsx', sheet_name='Ranked Measure Data', header=1, usecols=cols_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Years of Potential Life Lost Rate</th>\n",
       "      <th>% Fair or Poor Health</th>\n",
       "      <th>Average Number of Physically Unhealthy Days</th>\n",
       "      <th>Average Number of Mentally Unhealthy Days</th>\n",
       "      <th>% Low Birthweight</th>\n",
       "      <th>% Smokers</th>\n",
       "      <th>% Adults with Obesity</th>\n",
       "      <th>Food Environment Index</th>\n",
       "      <th>% Physically Inactive</th>\n",
       "      <th>% With Access to Exercise Opportunities</th>\n",
       "      <th>% Excessive Drinking</th>\n",
       "      <th>% Driving Deaths with Alcohol Involvement</th>\n",
       "      <th>Chlamydia Rate</th>\n",
       "      <th>Teen Birth Rate</th>\n",
       "      <th>Primary Care Physicians Rate</th>\n",
       "      <th>Dentist Rate</th>\n",
       "      <th>% With Annual Mammogram</th>\n",
       "      <th>% Vaccinated</th>\n",
       "      <th>High School Graduation Rate</th>\n",
       "      <th>% Some College</th>\n",
       "      <th>% Unemployed</th>\n",
       "      <th>% Children in Poverty</th>\n",
       "      <th>Income Ratio</th>\n",
       "      <th>% Single-Parent Households</th>\n",
       "      <th>Social Association Rate</th>\n",
       "      <th>Violent Crime Rate</th>\n",
       "      <th>Injury Death Rate</th>\n",
       "      <th>Average Daily PM2.5</th>\n",
       "      <th>Presence of Water Violation</th>\n",
       "      <th>% Severe Housing Problems</th>\n",
       "      <th>% Drive Alone to Work</th>\n",
       "      <th>% Long Commute - Drives Alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81791.0</td>\n",
       "      <td>9942.794666</td>\n",
       "      <td>22.028703</td>\n",
       "      <td>4.918052</td>\n",
       "      <td>4.939753</td>\n",
       "      <td>10.254871</td>\n",
       "      <td>20.927353</td>\n",
       "      <td>35.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>29.8</td>\n",
       "      <td>61.112287</td>\n",
       "      <td>13.903515</td>\n",
       "      <td>27.589171</td>\n",
       "      <td>614.1</td>\n",
       "      <td>30.894616</td>\n",
       "      <td>64.82388</td>\n",
       "      <td>48.18049</td>\n",
       "      <td>40.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>89.300000</td>\n",
       "      <td>60.411902</td>\n",
       "      <td>3.933567</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.261136</td>\n",
       "      <td>37.342739</td>\n",
       "      <td>12.371924</td>\n",
       "      <td>479.919182</td>\n",
       "      <td>82.250245</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.340702</td>\n",
       "      <td>85.794745</td>\n",
       "      <td>34.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>791.0</td>\n",
       "      <td>8128.591190</td>\n",
       "      <td>20.882987</td>\n",
       "      <td>4.743889</td>\n",
       "      <td>4.654031</td>\n",
       "      <td>8.619529</td>\n",
       "      <td>18.081557</td>\n",
       "      <td>33.3</td>\n",
       "      <td>7.2</td>\n",
       "      <td>34.7</td>\n",
       "      <td>69.130124</td>\n",
       "      <td>15.026031</td>\n",
       "      <td>26.785714</td>\n",
       "      <td>407.2</td>\n",
       "      <td>25.284927</td>\n",
       "      <td>45.04180</td>\n",
       "      <td>32.37352</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>62.009974</td>\n",
       "      <td>3.629079</td>\n",
       "      <td>19.3</td>\n",
       "      <td>5.234597</td>\n",
       "      <td>26.242679</td>\n",
       "      <td>12.071202</td>\n",
       "      <td>272.282220</td>\n",
       "      <td>68.526994</td>\n",
       "      <td>11.7</td>\n",
       "      <td>No</td>\n",
       "      <td>14.663462</td>\n",
       "      <td>86.523661</td>\n",
       "      <td>39.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>2967.0</td>\n",
       "      <td>7354.122530</td>\n",
       "      <td>17.509134</td>\n",
       "      <td>4.224519</td>\n",
       "      <td>4.304056</td>\n",
       "      <td>8.345003</td>\n",
       "      <td>17.489033</td>\n",
       "      <td>31.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>73.713549</td>\n",
       "      <td>17.958310</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>325.0</td>\n",
       "      <td>27.880692</td>\n",
       "      <td>72.89727</td>\n",
       "      <td>49.53629</td>\n",
       "      <td>43.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>86.361577</td>\n",
       "      <td>67.371620</td>\n",
       "      <td>3.615382</td>\n",
       "      <td>13.9</td>\n",
       "      <td>4.417767</td>\n",
       "      <td>24.139601</td>\n",
       "      <td>10.205617</td>\n",
       "      <td>203.660396</td>\n",
       "      <td>72.385054</td>\n",
       "      <td>10.3</td>\n",
       "      <td>No</td>\n",
       "      <td>13.566201</td>\n",
       "      <td>84.282433</td>\n",
       "      <td>41.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1005</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>472.0</td>\n",
       "      <td>10253.573403</td>\n",
       "      <td>29.591802</td>\n",
       "      <td>5.430279</td>\n",
       "      <td>5.185594</td>\n",
       "      <td>11.474559</td>\n",
       "      <td>21.999985</td>\n",
       "      <td>41.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>23.5</td>\n",
       "      <td>53.166770</td>\n",
       "      <td>12.844016</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>716.3</td>\n",
       "      <td>40.870815</td>\n",
       "      <td>31.65809</td>\n",
       "      <td>36.17218</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>81.410256</td>\n",
       "      <td>34.857649</td>\n",
       "      <td>5.171384</td>\n",
       "      <td>43.9</td>\n",
       "      <td>5.681410</td>\n",
       "      <td>56.603426</td>\n",
       "      <td>7.518797</td>\n",
       "      <td>414.277861</td>\n",
       "      <td>70.274612</td>\n",
       "      <td>11.5</td>\n",
       "      <td>No</td>\n",
       "      <td>14.583333</td>\n",
       "      <td>83.368470</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1007</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>471.0</td>\n",
       "      <td>11977.539484</td>\n",
       "      <td>19.439724</td>\n",
       "      <td>4.591482</td>\n",
       "      <td>4.551699</td>\n",
       "      <td>10.308710</td>\n",
       "      <td>19.114200</td>\n",
       "      <td>37.6</td>\n",
       "      <td>7.8</td>\n",
       "      <td>33.5</td>\n",
       "      <td>16.251364</td>\n",
       "      <td>15.570042</td>\n",
       "      <td>27.586207</td>\n",
       "      <td>339.7</td>\n",
       "      <td>41.696794</td>\n",
       "      <td>48.52656</td>\n",
       "      <td>22.32143</td>\n",
       "      <td>40.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>83.763838</td>\n",
       "      <td>44.137353</td>\n",
       "      <td>3.971828</td>\n",
       "      <td>27.8</td>\n",
       "      <td>4.368581</td>\n",
       "      <td>28.689236</td>\n",
       "      <td>8.381860</td>\n",
       "      <td>89.349126</td>\n",
       "      <td>108.156028</td>\n",
       "      <td>11.2</td>\n",
       "      <td>No</td>\n",
       "      <td>10.496454</td>\n",
       "      <td>84.910647</td>\n",
       "      <td>49.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS    State   County   Deaths  Years of Potential Life Lost Rate  \\\n",
       "0  1000  Alabama      NaN  81791.0                        9942.794666   \n",
       "1  1001  Alabama  Autauga    791.0                        8128.591190   \n",
       "2  1003  Alabama  Baldwin   2967.0                        7354.122530   \n",
       "3  1005  Alabama  Barbour    472.0                       10253.573403   \n",
       "4  1007  Alabama     Bibb    471.0                       11977.539484   \n",
       "\n",
       "   % Fair or Poor Health  Average Number of Physically Unhealthy Days  \\\n",
       "0              22.028703                                     4.918052   \n",
       "1              20.882987                                     4.743889   \n",
       "2              17.509134                                     4.224519   \n",
       "3              29.591802                                     5.430279   \n",
       "4              19.439724                                     4.591482   \n",
       "\n",
       "   Average Number of Mentally Unhealthy Days  % Low Birthweight  % Smokers  \\\n",
       "0                                   4.939753          10.254871  20.927353   \n",
       "1                                   4.654031           8.619529  18.081557   \n",
       "2                                   4.304056           8.345003  17.489033   \n",
       "3                                   5.185594          11.474559  21.999985   \n",
       "4                                   4.551699          10.308710  19.114200   \n",
       "\n",
       "   % Adults with Obesity  Food Environment Index  % Physically Inactive  \\\n",
       "0                   35.5                     5.8                   29.8   \n",
       "1                   33.3                     7.2                   34.7   \n",
       "2                   31.0                     8.0                   26.5   \n",
       "3                   41.7                     5.6                   23.5   \n",
       "4                   37.6                     7.8                   33.5   \n",
       "\n",
       "   % With Access to Exercise Opportunities  % Excessive Drinking  \\\n",
       "0                                61.112287             13.903515   \n",
       "1                                69.130124             15.026031   \n",
       "2                                73.713549             17.958310   \n",
       "3                                53.166770             12.844016   \n",
       "4                                16.251364             15.570042   \n",
       "\n",
       "   % Driving Deaths with Alcohol Involvement  Chlamydia Rate  Teen Birth Rate  \\\n",
       "0                                  27.589171           614.1        30.894616   \n",
       "1                                  26.785714           407.2        25.284927   \n",
       "2                                  30.769231           325.0        27.880692   \n",
       "3                                  40.000000           716.3        40.870815   \n",
       "4                                  27.586207           339.7        41.696794   \n",
       "\n",
       "   Primary Care Physicians Rate  Dentist Rate  % With Annual Mammogram  \\\n",
       "0                      64.82388      48.18049                     40.0   \n",
       "1                      45.04180      32.37352                     41.0   \n",
       "2                      72.89727      49.53629                     43.0   \n",
       "3                      31.65809      36.17218                     45.0   \n",
       "4                      48.52656      22.32143                     40.0   \n",
       "\n",
       "   % Vaccinated  High School Graduation Rate  % Some College  % Unemployed  \\\n",
       "0          41.0                    89.300000       60.411902      3.933567   \n",
       "1          41.0                    90.000000       62.009974      3.629079   \n",
       "2          44.0                    86.361577       67.371620      3.615382   \n",
       "3          37.0                    81.410256       34.857649      5.171384   \n",
       "4          38.0                    83.763838       44.137353      3.971828   \n",
       "\n",
       "   % Children in Poverty  Income Ratio  % Single-Parent Households  \\\n",
       "0                   23.9      5.261136                   37.342739   \n",
       "1                   19.3      5.234597                   26.242679   \n",
       "2                   13.9      4.417767                   24.139601   \n",
       "3                   43.9      5.681410                   56.603426   \n",
       "4                   27.8      4.368581                   28.689236   \n",
       "\n",
       "   Social Association Rate  Violent Crime Rate  Injury Death Rate  \\\n",
       "0                12.371924          479.919182          82.250245   \n",
       "1                12.071202          272.282220          68.526994   \n",
       "2                10.205617          203.660396          72.385054   \n",
       "3                 7.518797          414.277861          70.274612   \n",
       "4                 8.381860           89.349126         108.156028   \n",
       "\n",
       "   Average Daily PM2.5 Presence of Water Violation  % Severe Housing Problems  \\\n",
       "0                 11.0                         NaN                  14.340702   \n",
       "1                 11.7                          No                  14.663462   \n",
       "2                 10.3                          No                  13.566201   \n",
       "3                 11.5                          No                  14.583333   \n",
       "4                 11.2                          No                  10.496454   \n",
       "\n",
       "   % Drive Alone to Work  % Long Commute - Drives Alone  \n",
       "0              85.794745                           34.3  \n",
       "1              86.523661                           39.6  \n",
       "2              84.282433                           41.7  \n",
       "3              83.368470                           32.2  \n",
       "4              84.910647                           49.8  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the second part of the CHRRP dataset\n",
    "cols_2=[0,3,22,41,60,787,81,84,88,90,92,94,113,131,135,139,143,144,147,152,157,175,176,177,178,197,217,238,241,\n",
    "       245,248,249,250,252,254,256,258,260,262,264,267,269]\n",
    "c2_data=pd.read_excel('../data/2020_CHRD.xlsx', sheet_name='Additional Measure Data', header=1, usecols=cols_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename columns\n",
    "c2_data=c2_data.rename(columns={'% Uninsured.1':'% Children Uninsured', 'Segregation index':\n",
    "                                'Black/White Segregation Index', 'Segregation Index':\n",
    "                                'non-White/White Segregation Index','Average Grade Performance':\n",
    "                               'Average Reading Performance','Average Grade Performance.1':'Average Math Performance',\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the two CHRRP dataframes\n",
    "c_data=pd.merge(c1_data, c2_data, on='FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the state level rows (FIPS codes ending in 0)\n",
    "c_data=c_data.dropna(subset=['County'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the merged CHRRP dataset has all the right entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for any missing FIPS \n",
    "fips_unique=c_data['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alright, we're all good, let's keep merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "internet_data=pd.read_excel('../data/internet_data.xlsx', sheet_name='County Connections Dec 2017', usecols=[0,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=internet_data['countycode'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are missing two counties in the internet data: 2158 and 46102, so we will need to account for that when merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with the CHRRP data\n",
    "se_data=pd.merge(c_data, internet_data, how='left', left_on='FIPS', right_on='countycode')\n",
    "se_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know there are 2 missing rows in the internet data. Additionally, there are some entires where counties that have low numbers have -9999 instead of a value to 'preserve confidentiality'. We will need to interpolate these values.\n",
    "\n",
    "Note: some counties have a ration above one, presumably because some homes have multiple internet connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## interpolate the missing values\n",
    "se_data=se_data.replace(-9999,np.nan)\n",
    "con_mean=se_data['consumer'].mean()\n",
    "se_data['consumer']=se_data['consumer'].fillna(con_mean)\n",
    "non_con_mean=se_data['non_consumer'].mean()\n",
    "se_data['non_consumer']=se_data['non_consumer'].fillna(non_con_mean)\n",
    "ratio_mean=se_data['ratio'].mean()\n",
    "se_data['ratio']=se_data['ratio'].fillna(ratio_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=se_data['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, everything is here, let's do some final clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete the countycode column\n",
    "se_data=se_data.drop(columns=['countycode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the internet columns\n",
    "se_data=se_data.rename(columns={'consumer':'internet_consumer','non_consumer':'internet_nonconsumer','all':'internet_all',\n",
    "                    'hhs':'internet_hhs','ratio':'internet_ratio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add an internet percent column\n",
    "se_data['internet_percent']=100*se_data.internet_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### covid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data=pd.read_csv('../../nyt_data/live/us-counties.csv',usecols=['county','state','fips','cases','deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has the cummulative case and death counts for each US county up to the current date (whenever the data was last refreshed, currently September 6 2021)\n",
    "\n",
    "Let's check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=covid_data['fips'].unique()\n",
    "missing_fips(fips_unique)\n",
    "covid_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing 4 counties in Alaska and the individual New York City boros. This is because the NYT dataset combines 'Bristol Bay' and 'Lake and Penninsuala' county equivalents and 'Yakutat City' and 'Hoonah-Angoon' county equivalents and combines all boros into one NYC data point. We'll fix that.\n",
    "\n",
    "First, make a new data frame for the Alaska data. For simplicity we'll divide the cases and deaths by 2 for the counties of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alaska=[['Bristol Bay','Alaska',2060,int(covid_data.cases[covid_data['county']=='Bristol Bay plus Lake and Peninsula']/2)\n",
    "        ,int(covid_data.deaths[covid_data['county']=='Bristol Bay plus Lake and Peninsula']/2)],\n",
    "       ['Lake and Peninsula','Alaska',2164,int(covid_data.cases[covid_data['county']=='Bristol Bay plus Lake and Peninsula']/2)\n",
    "        ,int(covid_data.deaths[covid_data['county']=='Bristol Bay plus Lake and Peninsula']/2)],\n",
    "       ['Yakutat','Alaska',2282,int(covid_data.cases[covid_data['county']=='Yakutat plus Hoonah-Angoon']/2)\n",
    "        ,int(covid_data.deaths[covid_data['county']=='Yakutat plus Hoonah-Angoon']/2)],\n",
    "       ['Hoonah-Angoon','Alaska',2105,int(covid_data.cases[covid_data['county']=='Yakutat plus Hoonah-Angoon']/2)\n",
    "        ,int(covid_data.deaths[covid_data['county']=='Yakutat plus Hoonah-Angoon']/2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alaska_data=pd.DataFrame(alaska,columns=['county','state','fips','cases','deaths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alaska_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge it with the other covid data\n",
    "covid_data=covid_data.merge(alaska_data, how='outer')\n",
    "## drop the merged county rows\n",
    "covid_data=covid_data[covid_data['county']!='Bristol Bay plus Lake and Peninsula']\n",
    "covid_data=covid_data[covid_data['county']!='Yakutat plus Hoonah-Angoon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=covid_data['fips'].unique()\n",
    "missing_fips(fips_unique)\n",
    "covid_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do the NYC data. We will pull it from the New York City department of health database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_data=pd.read_csv('../../coronavirus-data/totals/by-boro.csv',usecols=['BOROUGH_GROUP','CASE_COUNT',\n",
    "                                                                         'DEATH_COUNT'])\n",
    "## drop the citywide datapoint\n",
    "nyc_data=nyc_data[nyc_data['BOROUGH_GROUP']!='Citywide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup the fips codes and names \n",
    "fips=[36005,36047,36061,36081,36085]\n",
    "state=['New York', 'New York',' New York', 'New York', 'New York']\n",
    "nyc_data['fips']=fips\n",
    "nyc_data['state']=state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_data=nyc_data.rename(columns={'CASE_COUNT':'cases','DEATH_COUNT':'deaths','BOROUGH_GROUP':'county'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge it \n",
    "covid_data=covid_data.merge(nyc_data, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=covid_data['fips'].unique()\n",
    "missing_fips(fips_unique)\n",
    "covid_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to merge the covid data into the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.merge(se_data, covid_data, left_on='FIPS', right_on='fips',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up the dataframe a bit \n",
    "data_clean=data.rename(columns={'cases':'covid_cases','deaths':'covid_deaths'})\n",
    "data_clean=data_clean.drop(columns=['county','state','fips'])\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we're all set, we have all the FIPS codes included and the expected number of counties. From now on, our central dataframe for all merged data will be called `data_clean`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severity Metric\n",
    "\n",
    "### obesity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the dataset\n",
    "obesity_data = pd.read_csv('../data/DiabetesAtlasCountyData.csv', skiprows=2, usecols=['CountyFIPS','Percentage'])\n",
    "obesity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there is missing data \n",
    "fips_unique=obesity_data['CountyFIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "obesity_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data so we can go ahead and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge\n",
    "data_clean=pd.merge(data_clean, obesity_data, left_on='FIPS', right_on='CountyFIPS',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean up the data by renaming `Percentage` to `%  Adults with Obesity` and dropping the original obesity data from CHRRP (outdated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean=data_clean.drop(columns=['% Adults with Obesity','CountyFIPS'])\n",
    "data_clean=data_clean.rename(columns={'Percentage':'% Adults with Obesity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data\n",
    "heart_disease=pd.read_csv('../data/all_heart.csv', usecols=['cnty_fips','Value','display_name'])\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=heart_disease['cnty_fips'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values so we can go ahead and merge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the data\n",
    "data_clean=pd.merge(data_clean, heart_disease, left_on='FIPS', right_on='cnty_fips',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up by changing column name and dropping extra geography data\n",
    "data_clean=data_clean.rename(columns={'Value':'Heart Disease Death Rate'})\n",
    "data_clean=data_clean.drop(columns=['cnty_fips','display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypertension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "hypertension=pd.read_csv('../data/hypertension.csv', usecols=['cnty_fips','Value','display_name'])\n",
    "hypertension.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=hypertension['cnty_fips'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nothing missing so we can merge\n",
    "data_clean=pd.merge(data_clean, hypertension, left_on='FIPS', right_on='cnty_fips',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleanup by dropping geography info and changing column name\n",
    "data_clean=data_clean.rename(columns={'Value':'Hypertension Death Rate'})\n",
    "data_clean=data_clean.drop(columns=['cnty_fips','display_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elderly population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the dataset \n",
    "pop_elderly=pd.read_csv('../data/elderly_agesex.csv', usecols=['POPESTIMATE','AGE65PLUS_TOT','Merge'])\n",
    "pop_elderly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is no FIPS code in this dataset so we'll have to add it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_elderly=pd.merge(pop_elderly, FIPS_codes, left_on='Merge', right_on='Merge',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_elderly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the extra geography info \n",
    "pop_elderly=pop_elderly.drop(columns=['State','County'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=pop_elderly['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing one entry, FIPS 35013 for Dona Ana County in New Mexico. We'll merge on the left to keep that row and then fill in the NaN with a 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add a column for % Adults 65 and Older\n",
    "pop_elderly['% Adults 65 and Older'] = (100 * pop_elderly['AGE65PLUS_TOT'] / pop_elderly['POPESTIMATE']).round(decimals=3)\n",
    "## drop extra columns \n",
    "pop_elderly=pop_elderly.drop(columns=['POPESTIMATE','AGE65PLUS_TOT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_elderly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the data\n",
    "data_clean=pd.merge(data_clean, pop_elderly, left_on='FIPS', right_on='FIPS',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the outdated %65 and over column and the extra geographic info\n",
    "data_clean=data_clean.drop(columns=['% 65 and over','Merge','State Abbrv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill in the NaN\n",
    "data_clean['% Adults 65 and Older']=data_clean['% Adults 65 and Older'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPD mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "copd_mortality=pd.read_csv('../data/IHME_USA_COUNTY.csv', usecols=['FIPS','Mortality Rate, 2014*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=copd_mortality['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no missing data so we can go ahead and merge\n",
    "## first clean up the data\n",
    "copd_mortality=copd_mortality.fillna(0)\n",
    "copd_mortality=copd_mortality.rename(columns={'Mortality Rate, 2014*':'COPD Mortality Rate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the data\n",
    "data_clean=pd.merge(data_clean, copd_mortality, left_on='FIPS', right_on='FIPS',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "diagnosed_diabetes=pd.read_csv('../data/2017_Diabetes_Diagnosed.csv', skiprows=2, usecols=['CountyFIPS','Percentage'])\n",
    "diagnosed_diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=diagnosed_diabetes['CountyFIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the data\n",
    "data_clean=pd.merge(data_clean, diagnosed_diabetes, left_on='FIPS', right_on='CountyFIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up by changing column names, removing outdated diabetes data, and dropping extra geographic info\n",
    "data_clean=data_clean.rename(columns={'Percentage':'% Diagnosed Diabetes'})\n",
    "data_clean=data_clean.drop(columns=['% Adults with Diabetes', 'CountyFIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Health Variables\n",
    "\n",
    "### Non-white population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in data and remove first row\n",
    "race_acs = pd.read_csv(\"../data/ACSDT5Y2018.B02001_data_with_overlays_2020-10-01T103850.csv\", usecols = [\"GEO_ID\", \"B02001_001E\", \"B02001_002E\"])\n",
    "race_acs = race_acs.loc[1:race_acs.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clean data and create % nonwhite variable\n",
    "race_acs.rename(columns = {\"GEO_ID\":\"FIPS\", \"B02001_001E\":\"Total\", \"B02001_002E\": \"White_Alone\"}, inplace = True)\n",
    "\n",
    "race_acs[\"FIPS\"] = race_acs[\"FIPS\"].apply(lambda x: x[9:14])\n",
    "race_acs = race_acs.loc[race_acs[\"FIPS\"] != \"\"]\n",
    "\n",
    "race_acs[\"Total\"] = race_acs[\"Total\"].astype(int)\n",
    "race_acs[\"White_Alone\"] = race_acs[\"White_Alone\"].astype(int)\n",
    "\n",
    "race_acs[\"Nonwhite\"] = race_acs[\"Total\"] - race_acs[\"White_Alone\"]\n",
    "race_acs[\"% Nonwhite\"] = 100 * (race_acs[\"Nonwhite\"] / race_acs[\"Total\"])\n",
    "race_acs = race_acs[[\"FIPS\", \"% Nonwhite\"]]\n",
    "race_acs['FIPS']=race_acs['FIPS'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_acs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=race_acs['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## looks good, merge to data_clean\n",
    "data_clean=pd.merge(data_clean, race_acs, left_on='FIPS', right_on='FIPS', how='left')\n",
    "data_clean[\"% Nonwhite\"].isnull().sum() / data_clean.shape[0] # Reviewing the percent of missing data in the merged column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Households w/o cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "hh_wo_car=pd.read_csv('../data/householdwoutcar_with_fips.csv',usecols=['fips', 'Number_Households_Wout_Car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=race_acs['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can go ahead and merge\n",
    "data_clean=pd.merge(data_clean, hh_wo_car, left_on=['FIPS'], right_on=['fips'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has number of households without cars but we want percentage so we'll divide by the number of households from the internet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['% households wo car']=data_clean['Number_Households_Wout_Car']/(data_clean['internet_hhs']*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up by dropping number of carless households and extra geography info\n",
    "data_clean=data_clean.drop(columns=['fips','Number_Households_Wout_Car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopsitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "hospitals = pd.read_csv('../data/Hospitals.csv', usecols=['ID', 'STATE', 'COUNTYFIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a list of all hospitals in the country so we'll need to get the counts for each county. There are also some hospitals with FIPS codes not available so we'll drop those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals=hospitals[hospitals['COUNTYFIPS']!='NOT AVAILABLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped = hospitals.groupby(['STATE', 'COUNTYFIPS'], \n",
    "                                      as_index=False).count().rename(columns={'ID':'Number of Hospitals'})\n",
    "hospitals_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know there are some counties without hospitals so we'll do a left merge and then fill those rows with 0. There are also two entries for 36091 and 47125, let's check those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped[hospitals_grouped['COUNTYFIPS']==36091]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped['COUNTYFIPS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped[hospitals_grouped['COUNTYFIPS']==39091]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 36091 it looks like one hospital in Ohio had a mistyped FIPS code (39091) so we'll add 1 to that county and drop the mistake row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped.at[1716,'Number of Hospitals']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped=hospitals_grouped.drop(1671)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped[hospitals_grouped['COUNTYFIPS']==47125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped[hospitals_grouped['COUNTYFIPS']==21125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for 47125 a hospital in Kentucky has a mistyped FIPS (21125) so we'll add 1 to that county and drop the mistake row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals_grouped.at[857,'Number of Hospitals']=2\n",
    "hospitals_grouped=hospitals_grouped.drop(895)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note using how=left to account for missing values (counties with no hospitals)\n",
    "hospitals_grouped['COUNTYFIPS']=hospitals_grouped['COUNTYFIPS'].astype(int)\n",
    "data_clean=pd.merge(data_clean, hospitals_grouped, left_on='FIPS', right_on='COUNTYFIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill in the NaNs with 0s\n",
    "data_clean['Number of Hospitals']=data_clean['Number of Hospitals'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop extra geography data\n",
    "data_clean=data_clean.drop(columns=['STATE','COUNTYFIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "col_name = 'Estimate!!Percent with a disability!!Subject!!Total civilian noninstitutionalized population'\n",
    "disability = pd.read_csv('../data/disability_data.csv', skiprows=1, usecols=['id', 'Geographic Area Name', col_name])\n",
    "disability = disability.rename(columns={col_name:'% disabled', 'Geographic Area Name':'Location'})\n",
    "disability.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the fips codes\n",
    "fips = []\n",
    "for county_id in disability['id']:\n",
    "    fips_i = county_id[-5:]\n",
    "#     print(fips)\n",
    "    fips.append(fips_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disability['FIPS'] = fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "disability['FIPS']=disability['FIPS'].astype(int)\n",
    "fips_unique=disability['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the data\n",
    "data_clean = pd.merge(data_clean, disability, left_on=['FIPS'], right_on=['FIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the extra geography info\n",
    "data_clean=data_clean.drop(['id','Location'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Health Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "health_insr = pd.read_csv(\"../data/sahie_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_insr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the full fips code and select only the data that is not segmented by race/age/sex\n",
    "health_insr[\"FIPS\"] = health_insr[\"statefips\"].astype(str).apply(lambda x: x if len(x) > 1 else \"0\"*(2-len(str(x))) + str(x) ) + \\\n",
    "    health_insr[\"countyfips\"].astype(str).apply(lambda x: x if len(str(x)) > 2 else  \"0\"*(3 - len(str(x))) + str(x))\n",
    "\n",
    "health_insr_subset = health_insr.loc[(health_insr[\"agecat\"] == 0)  & \\\n",
    "                                   (health_insr[\"racecat\"] == 0) & \\\n",
    "                                   (health_insr[\"sexcat\"] == 0) & \\\n",
    "                                   (health_insr[\"iprcat\"] == 0) & \\\n",
    "                                   (health_insr[\"countyfips\"] != 0) ]\n",
    "\n",
    "    \n",
    "health_insr_subset = health_insr_subset[[\"FIPS\", \"PCTUI\"]]\n",
    "health_insr_subset[\"PCTUI\"] = health_insr_subset[\"PCTUI\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "health_insr_subset = health_insr_subset.loc[health_insr_subset[\"PCTUI\"].isnull() == False]\n",
    "health_insr_subset.rename(columns = {\"PCTUI\":\"% Without Health Insurance\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "health_insr_subset['FIPS']=health_insr_subset['FIPS'].astype(int)\n",
    "fips_unique=health_insr_subset['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one missing FIPS code (15005, Kalawao Hawaii) so we'll do a left merge and fill in the NaN with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "data_clean=pd.merge(data_clean, health_insr_subset, left_on='FIPS', right_on='FIPS',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['% Without Health Insurance']=data_clean['% Without Health Insurance'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the outdated Unisured data from CHRRP\n",
    "data_clean=data_clean.drop(columns=['% Uninsured'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limited English Proficency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data\n",
    "limited_eng = pd.read_csv(\"../data/ACSST5Y2018.S1602_data_with_overlays.csv\", \\\n",
    "                          usecols = [\"GEO_ID\", \"S1602_C01_001E\", \"S1602_C03_001E\"])\n",
    "limited_eng = limited_eng.loc[1:limited_eng.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rename the columns and get the FIPS codes\n",
    "limited_eng.rename(columns = {\"GEO_ID\":\"FIPS\", \"S1602_C01_001E\":\"Total_Households\",\\\n",
    "                                  \"S1602_C03_001E\":\"Number_Limited_English_Speaking\"}, inplace = True)\n",
    "\n",
    "limited_eng[\"FIPS\"] = limited_eng[\"FIPS\"].apply(lambda x: x[9:14])\n",
    "limited_eng = limited_eng.loc[limited_eng[\"FIPS\"] != \"\"]\n",
    "\n",
    "limited_eng[\"Total_Households\"] = limited_eng[\"Total_Households\"].astype(int)\n",
    "limited_eng[\"Number_Limited_English_Speaking\"] = limited_eng[\"Number_Limited_English_Speaking\"].astype(int)\n",
    "limited_eng[\"% Limited English Proficiency\"] = 100 * (limited_eng[\"Number_Limited_English_Speaking\"] / limited_eng[\"Total_Households\"])\n",
    "\n",
    "limited_eng = limited_eng[[\"FIPS\", \"% Limited English Proficiency\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "limited_eng['FIPS']=limited_eng['FIPS'].astype(int)\n",
    "fips_unique=limited_eng['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all good, let's merge \n",
    "data_clean=pd.merge(data_clean, limited_eng, left_on='FIPS', right_on='FIPS',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commuting by Public Transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "public_transit=pd.read_csv('../data/ACS_S0801_Commute.csv', low_memory=False, usecols=['Merge','Estimate!!Total!!Workers 16 years and over!!MEANS OF TRANSPORTATION TO WORK!!Public transportation (excluding taxicab)'])\n",
    "public_transit=public_transit.rename(columns={\n",
    "                                              'Estimate!!Total!!Workers 16 years and over!!MEANS OF TRANSPORTATION TO WORK!!Public transportation (excluding taxicab)':'% workers commuting by public transit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the FIPS codes\n",
    "public_transit=pd.merge(public_transit, FIPS_codes, left_on='Merge', right_on='Merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_transit=public_transit.drop(columns=['State','County','Merge','State Abbrv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_transit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_unique=public_transit['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing one FIPS code (35013) so we'll merge left and fill in the NaN with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge the data\n",
    "data_clean=pd.merge(data_clean, public_transit, left_on='FIPS', right_on='FIPS',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill in the nan\n",
    "data_clean['% workers commuting by public transit']=data_clean['% workers commuting by public transit'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veterans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in data\n",
    "est_vets_percent =  pd.read_csv('../data/veteran_percent_est.csv', dtype={'FIPS': object}, index_col=0) # read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_vets_percent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "est_vets_percent['FIPS']=est_vets_percent['FIPS'].astype(int)\n",
    "fips_unique=est_vets_percent['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alll good so go ahead and merge \n",
    "data_clean = pd.merge(data_clean, est_vets_percent, on='FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opioid Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "opioids=pd.read_csv('../data/opioid_deaths_2018.txt',delimiter='\\t',usecols=['County Code','Deaths','Population',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opioids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop missing values\n",
    "opioids=opioids[opioids['Population']!='Missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the suppressed data with 5 (data is suppressed when there is <10 deaths in the county)\n",
    "opioids[\"Deaths\"]=opioids[\"Deaths\"].replace({'Suppressed':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the death rate and fill in the nans\n",
    "opioids['Deaths']=pd.to_numeric(opioids['Deaths'])\n",
    "opioids['Population']=pd.to_numeric(opioids['Population'])\n",
    "opioids.fillna(0)\n",
    "opioids['opioid death rate']=opioids['Deaths']/(opioids['Population']/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "#opioids['County Code']=opioids['County Code'].astype(int)\n",
    "fips_unique=opioids['County Code'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing 2 FIPS codes (02158 and 46102), we'll merge left and fill in the NaNs with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the extra columns and merge\n",
    "opioids=opioids.drop(columns=['Population','Deaths'])\n",
    "data_clean = pd.merge(data_clean, opioids, left_on='FIPS', right_on='County Code',how='left') # merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill in the nans and drop the extra geographic info\n",
    "data_clean['opioid death rate']=data_clean['opioid death rate'].fillna(0)\n",
    "data_clean=data_clean.drop(columns=['County Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic Harm Metric\n",
    "\n",
    "### Below Poverty Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in data\n",
    "poverty = pd.read_csv('../data/economic_harm/below_poverty.csv', usecols=['County ID', 'All Ages in Poverty Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=poverty['County ID'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all good so we can go ahead and merge \n",
    "data_clean = pd.merge(data_clean, poverty, left_on='FIPS', right_on='County ID',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleanup data frame\n",
    "data_clean = data_clean.rename(columns={'All Ages in Poverty Percent': '% Below Poverty'})\n",
    "data_clean=data_clean.drop(columns='County ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Income "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "income = pd.read_csv('../data/economic_harm/below_poverty.csv', usecols=['County ID', 'Median Household Income in Dollars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=income['County ID'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no missing FIPS so let's go ahead and merge (but first drop the outdated Median Household Income data)\n",
    "data_clean=data_clean.drop(columns=['Median Household Income'])\n",
    "data_clean = pd.merge(data_clean, income, left_on='FIPS', right_on='County ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleanup by renaming column and dropping extra fips column\n",
    "data_clean = data_clean.rename(columns={'Median Household Income in Dollars': 'Median Household Income'})\n",
    "data_clean=data_clean.drop(columns=['County ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No College Degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "college = pd.read_csv('../data/economic_harm/college.csv', usecols=[\"NAME\", \"S1501_C01_006E\", \"S1501_C01_011E\", \"S1501_C01_012E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean it up \n",
    "college = college.rename(columns={\"NAME\":\"County\", \"S1501_C01_006E\":\"Total Population 25+\", 'S1501_C01_011E':\"Associate's Degree\", \"S1501_C01_012E\":\"Bachelor's Degree\"})\n",
    "college = college.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset doesn't have FIPS codes so we'll have to merge them in. First split the County and State names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_split = college.County.str.split(\", \", expand=True)\n",
    "college_split.columns = ['county', 'state']\n",
    "college = pd.concat([college.drop(\"County\", axis=1), college_split], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college = pd.merge(college, FIPS_codes, left_on=['county', 'state'], right_on=['County', 'State'])  # Merge with FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the percentage with college degree\n",
    "college['% No College Degree'] = (college[\"Bachelor's Degree\"].astype('float64')/college[\"Total Population 25+\"].astype('float64')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the extra columns\n",
    "college=college.drop(columns=['Total Population 25+',\"Associate's Degree\",\"Bachelor's Degree\",'county',\n",
    "                             'state','State','County','Merge','State Abbrv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=college['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all good so we can go ahead and merge\n",
    "data_clean = pd.merge(data_clean, college, left_on='FIPS', right_on='FIPS',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data \n",
    "unemployed = pd.read_excel('../data/economic_harm/unemployed.xlsx', skiprows=5, skipfooter=3, usecols=['Unnamed: 3', 'Unnamed: 9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up the data \n",
    "unemployed_split = unemployed['Unnamed: 3'].str.split(\", \", expand=True)\n",
    "unemployed_split.columns = ['county', 'state']\n",
    "unemployed = pd.concat([unemployed.drop(\"Unnamed: 3\", axis=1), unemployed_split], axis=1)\n",
    "unemployed = unemployed.rename(columns = {'Unnamed: 9':'Unemployment Rate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployed.loc[(unemployed['county'] == 'District of Columbia'), 'state']  = 'DC'\n",
    "unemployed.loc[(unemployed.county == 'Anchorage Borough/municipality'), 'county'] = 'Anchorage Municipality'\n",
    "unemployed.loc[(unemployed.county == 'Juneau Borough/city'), 'county'] = 'Juneau City and Borough'\n",
    "unemployed.loc[(unemployed.county == 'Sitka Borough/city'), 'county'] = 'Sitka City and Borough'\n",
    "unemployed.loc[(unemployed.county == 'Wrangell Borough/city'), 'county'] = 'Wrangell City and Borough'\n",
    "unemployed.loc[(unemployed.county == 'Yakutat Borough/city'), 'county'] = 'Yakutat City and Borough'\n",
    "unemployed.loc[(unemployed.county == 'San Francisco County/city'), 'county'] = 'San Francisco County'\n",
    "unemployed.loc[(unemployed.county == 'Broomfield County/city'), 'county'] = 'Broomfield County'\n",
    "unemployed.loc[(unemployed.county == 'Denver County/city'), 'county'] = 'Denver County'\n",
    "unemployed.loc[(unemployed.county == 'Honolulu County/city'), 'county'] = 'Honolulu County'\n",
    "unemployed.loc[(unemployed.county == 'Nantucket County/town'), 'county'] = 'Nantucket County'\n",
    "unemployed.loc[(unemployed.county == 'Philadelphia County/city'), 'county'] = 'Philadelphia County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge in the FIPS codes\n",
    "unemployed = pd.merge(unemployed, FIPS_codes, left_on=['county', 'state'], right_on=['County', 'State Abbrv'])  # Merge with FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop the extra columns\n",
    "unemployed=unemployed.drop(columns=['county','state','State','County','Merge','State Abbrv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=unemployed['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing two FIPS so we'll merge left and fill NaN with 0\n",
    "## first drop the old Unemployed data\n",
    "data_clean=data_clean.drop(columns=['% Unemployed'])\n",
    "data_clean = pd.merge(data_clean, unemployed, left_on='FIPS', right_on='FIPS', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['Unemployment Rate']=data_clean['Unemployment Rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not In Labor Force\n",
    "\n",
    "We want the percentage of people above 16 that aren't in the labor force so we need to bring in population data too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in population 16+\n",
    "population = pd.read_csv('../data/economic_harm/population_2019.csv', usecols=['NAME', 'S0101_C01_025E']) # 16+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean it up \n",
    "population = population.rename(columns = {'NAME':'County', 'S0101_C01_025E':'Adult Population'})\n",
    "population = population.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the county and state and merge with fips codes\n",
    "population_split = population['County'].str.split(\", \", expand=True)\n",
    "population_split.columns = ['county', 'state']\n",
    "population = pd.concat([population.drop(\"County\", axis=1), population_split], axis=1)\n",
    "population = pd.merge(population, FIPS_codes, left_on=['county', 'state'], right_on=['County', 'State'])  # Merge with FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the labor data\n",
    "labor = pd.read_excel('../data/economic_harm/unemployed.xlsx', skiprows=5, skipfooter=3, usecols=['Unnamed: 3', 'Unnamed: 6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean it up: rename columns, split into state and county, rename counties as needed \n",
    "labor = labor.rename(columns={'Unnamed: 3': 'County', 'Unnamed: 6': 'Labor Force'})\n",
    "labor_split = labor['County'].str.split(\", \", expand=True)\n",
    "labor_split.columns = ['county', 'state']\n",
    "labor = pd.concat([labor.drop(\"County\", axis=1), labor_split], axis=1)\n",
    "labor.loc[(labor['county'] == 'District of Columbia'), 'state']  = 'DC'\n",
    "\n",
    "labor.loc[(labor.county == 'Anchorage Borough/municipality'), 'county'] = 'Anchorage Municipality'\n",
    "labor.loc[(labor.county == 'Juneau Borough/city'), 'county'] = 'Juneau City and Borough'\n",
    "labor.loc[(labor.county == 'Sitka Borough/city'), 'county'] = 'Sitka City and Borough'\n",
    "labor.loc[(labor.county == 'Wrangell Borough/city'), 'county'] = 'Wrangell City and Borough'\n",
    "labor.loc[(labor.county == 'Yakutat Borough/city'), 'county'] = 'Yakutat City and Borough'\n",
    "labor.loc[(labor.county == 'San Francisco County/city'), 'county'] = 'San Francisco County'\n",
    "labor.loc[(labor.county == 'Broomfield County/city'), 'county'] = 'Broomfield County'\n",
    "labor.loc[(labor.county == 'Denver County/city'), 'county'] = 'Denver County'\n",
    "labor.loc[(labor.county == 'Honolulu County/city'), 'county'] = 'Honolulu County'\n",
    "labor.loc[(labor.county == 'Nantucket County/town'), 'county'] = 'Nantucket County'\n",
    "labor.loc[(labor.county == 'Philadelphia County/city'), 'county'] = 'Philadelphia County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with the population data\n",
    "labor = pd.merge(labor, population, left_on=['county', 'state'], right_on=['County', 'State Abbrv'])  # Merge with FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate percentage not in labor force \n",
    "labor['% Not in Labor Force'] = (1 - (labor['Labor Force'].astype(float)/labor['Adult Population'].astype(float)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop extra columns\n",
    "labor = labor[['FIPS', '% Not in Labor Force']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=labor['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing two FIPS so we'll merge left and fill NaN with 0\n",
    "data_clean = pd.merge(data_clean, labor, left_on='FIPS', right_on='FIPS', how='left')\n",
    "data_clean['% Not in Labor Force']=data_clean['% Not in Labor Force'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobs in Tourism/Leisure/Hospitality\n",
    "\n",
    "We will need to do some calculations here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data and select only county info \n",
    "industries = pd.read_excel('../data/economic_harm/industries.xlsx')\n",
    "industries = industries.loc[(industries['Area Type'] == 'County')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select only the leisure and hospitality data and the full employment data\n",
    "totals = industries.loc[(industries['Industry'] == '10 Total, all industries') & (industries['Ownership'] == 'Total Covered')]\n",
    "leisure = industries.loc[(industries['Industry'] == '1026 Leisure and hospitality')]\n",
    "leisure = pd.merge(leisure, totals, left_on='Area\\nCode', right_on='Area\\nCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the percentage in leisure and hospitality and drop extra info\n",
    "leisure['% Jobs in Leisure and Hospitality'] = leisure['Annual Average Employment_x']/leisure['Annual Average Employment_y']*100\n",
    "leisure = leisure[['Area\\nCode', '% Jobs in Leisure and Hospitality']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "leisure['Area\\nCode']=leisure['Area\\nCode'].astype(int)\n",
    "fips_unique=leisure['Area\\nCode'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we're missing a few fips codes so we'll merge left and fill NaN with 0\n",
    "## missing two FIPS so we'll merge left and fill NaN with 0\n",
    "data_clean = pd.merge(data_clean, leisure, left_on='FIPS', right_on='Area\\nCode', how='left')\n",
    "data_clean = data_clean.drop(columns = ['Area\\nCode'])\n",
    "data_clean['% Jobs in Leisure and Hospitality']=data_clean['% Jobs in Leisure and Hospitality'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Time Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data\n",
    "part_time = pd.read_csv('../data/economic_harm/part-time.csv', usecols=['NAME', 'S2303_C01_001E', 'S2303_C01_009E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean it up \n",
    "part_time = part_time.rename(columns={'NAME':'County', 'S2303_C01_001E':'Total Population', 'S2303_C01_009E':'Not Part-Time'})\n",
    "part_time = part_time.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate part time percentage \n",
    "part_time['% Part-time'] = 100 - (part_time['Not Part-Time'].astype(float)/part_time['Total Population'].astype(float))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge in the FIPS codes (first split into county and state)\n",
    "part_time_split = part_time.County.str.split(\", \", expand=True)\n",
    "part_time_split.columns = ['county', 'state']\n",
    "part_time=pd.concat([part_time.drop('County', axis=1), part_time_split], axis=1)\n",
    "part_time = pd.merge(part_time, FIPS_codes, left_on=['county', 'state'], right_on=['County', 'State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop extra columns\n",
    "part_time=part_time[['FIPS','% Part-time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=part_time['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all good so we can go ahead and merge\n",
    "data_clean = pd.merge(data_clean, part_time, left_on=['FIPS'], right_on=['FIPS'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Employed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in the data\n",
    "self_employed = pd.read_csv('../data/economic_harm/self-employed.csv', usecols=['NAME', 'S2408_C01_001E', 'S2408_C01_004E', 'S2408_C01_009E'])\n",
    "# Including Self-employed in own incorporated business workers and Self-employed in own non-incorporated business workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean it up \n",
    "self_employed = self_employed.rename(columns={'NAME':'County', 'S2408_C01_001E':'Total Population', 'S2408_C01_004E':'Self Employed 1', 'S2408_C01_009E':'Self Employed 2'})\n",
    "self_employed = self_employed.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge with FIPS codes, first split the county column \n",
    "self_employed_split = self_employed.County.str.split(\", \", expand=True)\n",
    "self_employed_split.columns = ['county', 'state']\n",
    "self_employed = pd.concat([self_employed.drop(\"County\", axis=1), self_employed_split], axis=1)\n",
    "self_employed = pd.merge(self_employed, FIPS_codes, left_on=['county', 'state'], right_on=['County', 'State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate self employed percentage\n",
    "self_employed['% Self-Employed'] = (self_employed['Self Employed 1'].astype(float) + self_employed['Self Employed 2'].astype(float))/self_employed['Total Population'].astype(float) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop extra columns\n",
    "self_employed=self_employed[['FIPS','% Self-Employed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "self_employed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=self_employed['FIPS'].unique()\n",
    "missing_fips(fips_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## all good so we can go ahead an merge\n",
    "data_clean = pd.merge(data_clean, self_employed, left_on=['FIPS'], right_on=['FIPS'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for missing values\n",
    "fips_unique=data_clean['FIPS'].unique()\n",
    "missing_fips(fips_unique)\n",
    "data_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some checks to quickly assess the quality of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in the CHRRP dataset and still a few in our added variables. We'll leave the CHRRP data alone but fill in the additional variables NaN values with 0 (`%households wo car`, `% Below Poverty`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['% households wo car']=data_clean['% households wo car'].fillna(0)\n",
    "data_clean['% Below Poverty']=data_clean['% Below Poverty'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add the 0s back to the FIPS codes so they can be used with plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_clean['FIPS']=data_clean['FIPS'].apply(fips_zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll list all the included variables and save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save full data set\n",
    "data_clean.to_csv('../data/merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
